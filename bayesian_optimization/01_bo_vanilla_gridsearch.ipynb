{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook uses a vanilla (basic) Bayesian optimization algorithm to tackle an urban travel demand (i.e., origin-destination, OD) calibration problem. The traffic simulations are based on the SUMO simulator.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SUMO configuration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mount GDrive\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are working w/ colab rather than a jupyterlab notebook this drive mounting and sumo installation will need to be done every time you restart the runtime.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setup venv**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %python -m venv .venv\n",
    "# %source .venv/bin/activate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install SUMO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %sudo add-apt-repository -y ppa:sumo/stable\n",
    "# %sudo apt-get update\n",
    "# %sudo apt-get -y install sumo sumo-tools sumo-doc &"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set sumo env vars\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set environment variable\n",
    "import os\n",
    "import sys\n",
    "os.environ['SUMO_HOME'] = '/usr/share/sumo'\n",
    "os.environ['LIBSUMO_AS_TRACI'] = '1' #Optional: for a huge performance boost (~8x) with Libsumo (No GUI)\n",
    "\n",
    "if \"SUMO_HOME\" in os.environ:\n",
    "    tools = os.path.join(os.environ[\"SUMO_HOME\"], \"tools\")\n",
    "    sys.path.append(tools)\n",
    "else:\n",
    "    sys.exit(\"Please declare the environment variable 'SUMO_HOME'\")\n",
    "#import traci\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Macros / utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"/app\"\n",
    "\n",
    "# if base_path has a space in it, the sumo code will not work\n",
    "if ' ' in base_path:\n",
    "    raise ValueError(\"base_path should not contain spaces\")\n",
    "\n",
    "os.chdir(base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install missing packages\n",
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import multiprocessing as mp\n",
    "\n",
    "import torch\n",
    "from torch.quasirandom import SobolEngine\n",
    "\n",
    "from botorch import fit_gpytorch_mll\n",
    "from botorch.acquisition import qLogExpectedImprovement\n",
    "from botorch.models import SingleTaskGP\n",
    "from botorch.models.transforms import Standardize\n",
    "from botorch.optim import optimize_acqf\n",
    "from botorch.sampling.stochastic_samplers import StochasticSampler\n",
    "from botorch.utils.transforms import unnormalize, normalize\n",
    "\n",
    "from gpytorch.constraints import Interval\n",
    "from gpytorch.kernels import MaternKernel, ScaleKernel\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "\n",
    "\n",
    "from bayesian_optimization.helpers import (load_kwargs_config, \n",
    "                    compute_nrmse_counts_all_edges, \n",
    "                    parse_loop_data_xml_to_pandas, \n",
    "                    create_taz_xml,\n",
    "                    simulate_od,\n",
    "                    od_xml_to_df,\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_kwargs_config(base_path, \"gridsearch\")\n",
    "Path(config[\"simulation_run_path\"]).mkdir(parents=True, exist_ok=True)\n",
    "pprint.pprint(dict(config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create GT (ground truth) scenario\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Ground Truth OD + fixed routes\n",
    "print(f\"Reading: {config['file_gt_od']}\")\n",
    "gt_od_df = od_xml_to_df(config[\"file_gt_od\"])\n",
    "\n",
    "print(f\"Reading: {config['fixed_routes']}\")\n",
    "routes_df = pd.read_csv(config[\"fixed_routes\"], index_col=0)\n",
    "\n",
    "# if config[\"edge_selection\"] exists\n",
    "if \"edge_selection\" in config:\n",
    "    if not os.path.exists(config[\"edge_selection\"]):\n",
    "        edge_selection = None\n",
    "    else:\n",
    "        print(f\"Reading: {config['edge_selection']}\")\n",
    "        edge_selection = pd.read_csv(config[\"edge_selection\"], header=None)\n",
    "        edge_selection.columns = [\"edge_id\"]\n",
    "        edge_selection = edge_selection[\"edge_id\"].tolist()\n",
    "else:\n",
    "    edge_selection = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulate the GT scenario to obtain the GT traffic statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulation_gt_run_path =f'{config[\"simulation_run_path\"]}/ground_truth'\n",
    "prefix_output_gt = f'{simulation_gt_run_path}/sim'\n",
    "sim_edge_out_gt = f'{prefix_output_gt}_{config[\"EDGE_OUT_STR\"]}'\n",
    "new_od_xml = f'{simulation_gt_run_path}/od.xml'\n",
    "\n",
    "Path(simulation_gt_run_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "base_od = gt_od_df.copy()\n",
    "gt_od_vals = gt_od_df['count'].astype(float).to_numpy()\n",
    "curr_od = gt_od_vals.copy()\n",
    "base_od['count'] = curr_od\n",
    "base_od = base_od.rename(columns={'fromTaz':'from', 'toTaz':'to'})        \n",
    "create_taz_xml(new_od_xml, base_od, config[\"od_duration_sec\"], base_path)\n",
    "print(base_od)\n",
    "\n",
    "# Run simulation\n",
    "simulate_od(new_od_xml, \n",
    "            prefix_output_gt, \n",
    "            base_path, \n",
    "            config[\"net_xml\"], \n",
    "            config[\"taz2edge_xml\"], \n",
    "            config[\"additional_xml\"],\n",
    "            routes_df,\n",
    "            config[\"sim_end_time\"],\n",
    "            config[\"TRIPS2ODS_OUT_STR\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read and process the GT simulation outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_edge_gt, _, _ = parse_loop_data_xml_to_pandas(base_path, sim_edge_out_gt, prefix_output_gt, config[\"SUMO_PATH\"], edge_list=edge_selection)\n",
    "# picking at edges as GT edges\n",
    "num_gt_edges = df_edge_gt.shape[0]\n",
    "print(\"Number of GT edges:\",num_gt_edges)\n",
    "gt_edge_data = df_edge_gt\\\n",
    "    .sort_values(by=['interval_nVehContrib'], ascending=False)\\\n",
    "    .iloc[:num_gt_edges]\n",
    "\n",
    "print(sim_edge_out_gt)\n",
    "print(gt_edge_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayesian optimization utils / helpers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "print(device)\n",
    "dtype = torch.double\n",
    "\n",
    "dim_od = gt_od_df.shape[0]\n",
    "print(dim_od)\n",
    "\n",
    "bounds = torch.tensor([\n",
    "    [ 0 for _ in range(dim_od)],\n",
    "    [ 2000 for _ in range(dim_od)]\n",
    "], device=device, dtype=dtype) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and simulate a full-grid search.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full grid search\n",
    "n_full_search = 201\n",
    "candidates = []\n",
    "\n",
    "# print(dim_od)\n",
    "for i in range(dim_od):\n",
    "    candidates.append(torch.linspace(0,1,n_full_search))\n",
    "\n",
    "search_space = torch.meshgrid(candidates,indexing=\"ij\")\n",
    "search_space = torch.stack(search_space , 0)\n",
    "search_space.shape\n",
    "search_space = search_space.view(dim_od, -1)\n",
    "search_space = search_space.transpose(0,1)\n",
    "search_space = search_space.to(device)\n",
    "\n",
    "# map the normalized into the original parameter space\n",
    "train_X0 = unnormalize(search_space, bounds)\n",
    "train_X0 = train_X0[1:,:]\n",
    "\n",
    "# delete rows if contains 0\n",
    "train_X0 = train_X0[~(train_X0 == 0).any(axis=1)]\n",
    "\n",
    "print(f\"train_X0 shape = {train_X0.shape}\")\n",
    "print(train_X0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "# Function to write logs safely\n",
    "def log_results(file_handle, log_data, lock):\n",
    "    with lock:  # Use a lock to ensure safe writing to the file\n",
    "        file_handle.write(log_data + '\\n')\n",
    "        file_handle.flush()  # Ensure data is written immediately\n",
    "\n",
    "def run_simulation(od_matrix, i, config, gt_od_df, base_path, routes_df, gt_edge_data, edge_selection, lock):\n",
    "    print(f\"########### OD: {i} ###########\")\n",
    "    print(od_matrix)\n",
    "    \n",
    "    simulation_run_path_grid = f'{config[\"simulation_run_path\"]}/full_search'\n",
    "    Path(simulation_run_path_grid).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    new_od_xml = f\"{simulation_run_path_grid}/grid_od_{config['network_name']}_{i}.xml\"\n",
    "    prefix_output_grid = f'{simulation_run_path_grid}/grid_{i}'\n",
    "    \n",
    "    curr_od = np.array(od_matrix)\n",
    "    \n",
    "    print(f'Total expected GT demand: {np.sum(curr_od)}')\n",
    "\n",
    "    # Create OD XML file\n",
    "    base_od = gt_od_df.copy()\n",
    "    base_od['count'] = curr_od\n",
    "    base_od['count'] = [round(elem, 1) for elem in base_od['count']]     \n",
    "    base_od = base_od.rename(columns={'fromTaz': 'from', 'toTaz': 'to'})        \n",
    "    create_taz_xml(new_od_xml, base_od, config[\"od_duration_sec\"], base_path)\n",
    "    \n",
    "    # Simulate OD\n",
    "    simulate_od(new_od_xml, \n",
    "                prefix_output_grid, \n",
    "                base_path, \n",
    "                config[\"net_xml\"], \n",
    "                config[\"taz2edge_xml\"], \n",
    "                config[\"additional_xml\"],\n",
    "                routes_df,\n",
    "                config[\"sim_end_time\"],\n",
    "                config[\"TRIPS2ODS_OUT_STR\"])\n",
    "    \n",
    "    # Compute loss\n",
    "    sim_edge_out = f'{base_path}/{prefix_output_grid}_{config[\"EDGE_OUT_STR\"]}'\n",
    "    curr_loop_stats, _, _ = parse_loop_data_xml_to_pandas(base_path, sim_edge_out, prefix_output_grid, config[\"SUMO_PATH\"], edge_list=edge_selection)\n",
    "    curr_loss = compute_nrmse_counts_all_edges(gt_edge_data, curr_loop_stats)\n",
    "    \n",
    "    # Open log file in append mode inside each process\n",
    "    log_file_path = f'{config[\"simulation_run_path\"]}/simulation_log.txt'\n",
    "    with open(log_file_path, 'a') as log_file:\n",
    "        # Log result safely to file in CSV format\n",
    "        log_data = f\"{i}, {curr_loss}, \" + ', '.join(map(str, curr_od))\n",
    "        log_results(log_file, log_data, lock)\n",
    "    \n",
    "    # Return the loss and OD matrix\n",
    "    return curr_od, curr_loss\n",
    "\n",
    "def parallel_sumo_runs(train_X0, config, gt_od_df, base_path, routes_df, gt_edge_data, edge_selection):\n",
    "    ods_epsilon = []\n",
    "    loss_all = []\n",
    "    batch_data_i = torch.ones((train_X0.shape[0], train_X0.shape[1] + 1)).to(train_X0.device) * np.nan\n",
    "    train_X0_list = train_X0.tolist()\n",
    "\n",
    "    # Set up multiprocessing manager for shared lock\n",
    "    manager = mp.Manager()\n",
    "    lock = manager.Lock()\n",
    "\n",
    "    # Set up multiprocessing pool\n",
    "    pool = mp.Pool(processes=mp.cpu_count())  # Use all available CPUs\n",
    "\n",
    "    results = [pool.apply_async(run_simulation, args=(\n",
    "        train_X0_list[i], \n",
    "        i, \n",
    "        config, \n",
    "        gt_od_df, \n",
    "        base_path, \n",
    "        routes_df, \n",
    "        gt_edge_data, \n",
    "        edge_selection, \n",
    "        lock)) for i in range(len(train_X0_list))]\n",
    "    \n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    return ods_epsilon, loss_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_sumo_runs(train_X0, config, gt_od_df, base_path, routes_df, gt_edge_data, edge_selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_file_path = f'{config[\"simulation_run_path\"]}/simulation_log.txt'\n",
    "data = pd.read_csv(log_file_path, header=None)\n",
    "len_col = data.shape[1]\n",
    "data.columns = [\"od_index\", \"loss\"] + [f\"od_{i}\" for i in range(len_col-2)]\n",
    "data.iloc[:,2:] = data.iloc[:,2:].map(lambda x: round(x, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw a heatmap\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# data.od_0 : x-axis\n",
    "# data.od_1 : y-axis\n",
    "# data.loss : color\n",
    "data_temp = data.copy()\n",
    "# filter 250-750 and 650-1150\n",
    "data_temp = data_temp[(data_temp['od_0'] >= 250) & (data_temp['od_0'] <= 750)]\n",
    "data_temp = data_temp[(data_temp['od_1'] >= 650) & (data_temp['od_1'] <= 1150)]\n",
    "heatmap_data = data_temp.pivot_table(index='od_0', columns='od_1', values='loss')\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.heatmap(heatmap_data, cmap='coolwarm', annot=False)\n",
    "plt.title('NRMSE heatmap')\n",
    "\n",
    "# save the heatmap\n",
    "plt.savefig(f'{config[\"simulation_run_path\"]}/heatmap.png')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
