{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "from torch.quasirandom import SobolEngine\n",
    "\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "from gpytorch.kernels import MaternKernel, ScaleKernel\n",
    "from gpytorch.constraints import Interval\n",
    "\n",
    "from botorch import fit_gpytorch_mll\n",
    "from botorch.utils.transforms import unnormalize, normalize\n",
    "from botorch.models import SingleTaskGP\n",
    "from botorch.models.transforms import Standardize\n",
    "from botorch.optim import optimize_acqf\n",
    "from botorch.acquisition import qLogExpectedImprovement\n",
    "from botorch.sampling.stochastic_samplers import StochasticSampler\n",
    "\n",
    "from helpers import (load_experiment_metadata, \n",
    "                    compute_nrmse_counts_all_edges, \n",
    "                    parse_loop_data_xml_to_pandas, \n",
    "                    create_taz_xml,\n",
    "                    simulate_od,\n",
    "                    od_xml_to_df,\n",
    "                    xml2df_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!Pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_path = \"/home/bench/Gitsrcs/origin_destination_bayes_opt\"\n",
    "# base_path = \"/Users/osorio/HEC/Research/Group/FacultyCollaborations/SeongjinChoi_UMN/Code_BO/origin_destination_bayes_opt-main\"\n",
    "base_path = \"/Users/chois/Gitsrcs/origin_destination_bayes_opt\"\n",
    "os.chdir(base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = Path(base_path , 'config')\n",
    "print(f\"config_path: {config_path}\")\n",
    "\n",
    "config, sim_setup = load_experiment_metadata(config_path)\n",
    "\n",
    "network_name = sim_setup['network_name']\n",
    "model_name = sim_setup['model_name']\n",
    "\n",
    "network_path = Path(\"network\" , network_name)\n",
    "taz2edge_xml = Path(base_path, network_path, 'taz.xml')\n",
    "net_xml = Path(base_path, network_path, 'net.xml')\n",
    "fixed_routes = Path(base_path, network_path, 'routes.csv')\n",
    "file_gt_od = Path(base_path, network_path, 'od.xml')\n",
    "additional_xml = Path(base_path, network_path, 'additional.xml')\n",
    "\n",
    "out_path = f\"output/{network_name}_{model_name}\" \n",
    "Path(out_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "gt_version_str = network_name           ## TODO : need to check if this is correct\n",
    "\n",
    "EDGE_OUT_STR = f'edge_data_{network_name}.xml'\n",
    "# suffix of simulation output edge file\n",
    "TRIPS2ODS_OUT_STR = 'trips.xml'\n",
    "SUMO_PATH = config[\"SUMO\"]\n",
    "\n",
    "sim_start_time = sim_setup['sim_start_time']\n",
    "sim_end_time = sim_setup['sim_end_time']\n",
    "sim_stat_freq_sec = sim_setup['sim_stat_freq_sec']\n",
    "od_duration_sec = sim_setup['od_duration_sec']\n",
    "\n",
    "n_init_search = sim_setup['n_init_search']\n",
    "\n",
    "NITER = sim_setup[\"BO_niter\"]\n",
    "BATCH_SIZE = sim_setup[\"BO_batch_size\"]\n",
    "NUM_RESTARTS = sim_setup[\"BO_num_restarts\"]\n",
    "RAW_SAMPLES = sim_setup[\"BO_raw_samples\"] \n",
    "\n",
    "simulation_run_path =f'{out_path}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # taz2edge_xml = 'taz_new.xml'\n",
    "# # net_xml = 'SFO.net.xml'\n",
    "# # fixed_routes_xml = f'{base_path}/5hr_route_choice_set.csv'\n",
    "# # od_duration_seconds = 5*60 \n",
    "\n",
    "# # # duration of sample time for simulation output statistics\n",
    "# # simulation_stat_freq_sec = od_duration_seconds\n",
    "# # sim_end_time = od_duration_seconds\n",
    "# # additional_xml = f'additional.add_statfreq{od_duration_seconds}.xml'\n",
    "\n",
    "# # # suffix of simulation output edge file\n",
    "# # EDGE_OUT_STR = 'edge_data_SFO.xml'\n",
    "# # TRIPS2ODS_OUT_STR = 'trips.xml'\n",
    "# # SUMO_PATH = '/usr/local/opt/sumo/share/sumo'\n",
    "\n",
    "# od_duration_seconds = 30*60 \n",
    "\n",
    "# # duration of sample time for simulation output statistics\n",
    "# simulation_stat_freq_sec = od_duration_seconds\n",
    "# sim_end_time = od_duration_seconds\n",
    "\n",
    "# # TODO: it might be cleaner to replace this with a config file, i attached to my email an example. and one can define one config file per network. \n",
    "# network_name = \"quickstart\"\n",
    "# model_name = \"bo_vanilla\"\n",
    "\n",
    "# network_path = f\"network/{network_name}\"\n",
    "# taz2edge_xml = f\"{base_path}/{network_path}/taz.xml\"\n",
    "# net_xml = f\"{base_path}/{network_path}/net.xml\"\n",
    "# fixed_routes = f\"{base_path}/{network_path}/routes.csv\"\n",
    "# # od_xml = f\"{network_path}/od.xml\"       ## TODO : need to check if this is correct\n",
    "# file_gt_od = f\"{base_path}/{network_path}/od.xml\"      ## TODO : need to check if this is correct\n",
    "# # file_gt_edges                         ## TODO : need to check if this is necessary (not being used below)\n",
    "# additional_xml = f\"{base_path}/{network_path}/additional.xml\"\n",
    "# out_path = f\"output/{network_name}_{model_name}\"\n",
    "# out_path = f\"output/{network_name}_{model_name}\"       ## TODO : need to check if this is correct\n",
    "# # prefix_output = f\"{out_path}/out\"     ## TODO : need to check if this is correct\n",
    "# gt_version_str = network_name           ## TODO : need to check if this is correct\n",
    "\n",
    "# EDGE_OUT_STR = f'edge_data_{network_name}.xml'\n",
    "# # suffix of simulation output edge file\n",
    "# TRIPS2ODS_OUT_STR = 'trips.xml'\n",
    "# # TODO I changed this path for it to work for me.\n",
    "# SUMO_PATH = '/opt/homebrew/opt/sumo/share/sumo'\n",
    "# #SUMO_PATH = \"/usr/share/sumo\"\n",
    "\n",
    "# Path(out_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_version_str = 'v4'\n",
    "\n",
    "# gt v4:\n",
    "mean_od_val = 100\n",
    "num_ods = 10\n",
    "\n",
    "print('if you want to optimize them all (~86k) set num_ods as defined in commented line below')\n",
    "#num_ods = routes_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# od_xml = f'gt_od_{gt_version_str}.xml'\n",
    "# file_gt = f'{base_path}/gt_od_{gt_version_str}.xml'\n",
    "# file_gt_edges = f'{base_path}/gt_edges_{gt_version_str}.csv'\n",
    "# prefix_output_gt = f'gt_{gt_version_str}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get GT OD\n",
    "print(\"Reading:\",file_gt_od)\n",
    "tree = ET.parse(file_gt_od)\n",
    "root = tree.getroot()\n",
    "gt_od_df =  xml2df_str(root, 'tazRelation')\n",
    "\n",
    "gt_od_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Reading:\",fixed_routes)\n",
    "routes_df = pd.read_csv(fixed_routes, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_od_df = od_xml_to_df(file_gt_od)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_od_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanilla BO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declare parameter space\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: let's put all import  statements at the top of the notebook\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "dtype = torch.double\n",
    "\n",
    "### Declare search space\n",
    "# dimensionality of input space\n",
    "\n",
    "dim_od = gt_od_df.shape[0]\n",
    "print(dim_od)\n",
    "\n",
    "#bounds = torch.tensor([\n",
    "#    [ gt_od_df['count'].astype(float).min() - 2 for _ in range(dim_od)],\n",
    "#    [ gt_od_df['count'].astype(float).max() + 2 for _ in range(dim_od)]\n",
    "#], device=device, dtype=dtype) \n",
    "\n",
    "bounds = torch.tensor([\n",
    "    [ 0 for _ in range(dim_od)],\n",
    "    [ 2000 for _ in range(dim_od)]\n",
    "], device=device, dtype=dtype) \n",
    "\n",
    "\n",
    "bounds\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run GT simulation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulation_gt_run_path =f'{out_path}/ground_truth'\n",
    "Path(simulation_gt_run_path).mkdir(parents=True, exist_ok=True)\n",
    "prefix_output_gt = f'{simulation_gt_run_path}/sim'\n",
    "\n",
    "sim_edge_out_gt = f'{prefix_output_gt}_{EDGE_OUT_STR}'\n",
    "new_od_xml = f'{simulation_gt_run_path}/od.xml'\n",
    "\n",
    "base_od = gt_od_df.copy()\n",
    "gt_od_vals = gt_od_df['count'].astype(float).to_numpy()\n",
    "curr_od = gt_od_vals.copy()\n",
    "base_od['count'] = curr_od\n",
    "base_od = base_od.rename(columns={'fromTaz':'from', 'toTaz':'to'})        \n",
    "create_taz_xml(new_od_xml, base_od, od_duration_sec, base_path)\n",
    "\n",
    "print(base_od)\n",
    "\n",
    "# Run simulation\n",
    "\n",
    "simulate_od(new_od_xml, \n",
    "            prefix_output_gt, \n",
    "            base_path, \n",
    "            net_xml, \n",
    "            taz2edge_xml, \n",
    "            additional_xml,\n",
    "            routes_df,\n",
    "            sim_end_time,\n",
    "            TRIPS2ODS_OUT_STR)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read output of GT simulation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_edge_gt, _, _ = parse_loop_data_xml_to_pandas(base_path, sim_edge_out_gt, prefix_output_gt,SUMO_PATH)\n",
    "# picking at edges as GT edges\n",
    "num_gt_edges = df_edge_gt.shape[0]\n",
    "print(\"Number of GT edges:\",num_gt_edges)\n",
    "gt_edge_data = df_edge_gt\\\n",
    "    .sort_values(by=['interval_nVehContrib'], ascending=False)\\\n",
    "    .iloc[:num_gt_edges]\n",
    "\n",
    "# gt_edge_data.shape\n",
    "print(sim_edge_out_gt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_edge_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample according to Sobol\n",
    "sobol = SobolEngine(dim_od, scramble=True)\n",
    "x_0 = sobol.draw(n_init_search).to(dtype=dtype).to(device)\n",
    "print(x_0.shape)\n",
    "\n",
    "\n",
    "# map the normalized into the original parameter space\n",
    "train_X0 = unnormalize(x_0, bounds)\n",
    "print(train_X0.shape)\n",
    "train_X0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#num_epsilon_iter = 2\n",
    "ods_epsilon = []\n",
    "loss_all = []\n",
    "batch_data_i = []\n",
    "\n",
    "# Base OD which we will update their count entries\n",
    "base_od = gt_od_df.copy()\n",
    "gt_od_vals = gt_od_df['count'].astype(float).to_numpy()\n",
    "\n",
    "for i , x in enumerate(train_X0.tolist()):\n",
    "#for i , x in enumerate(\n",
    "#      [[ 94.66438596,  91.97375804, 101.82277249, 112.44778006,\n",
    "#            105.33019264,  92.62166575,  99.8673423 ,  93.71928772,\n",
    "#            116.16658554,  94.79717515],\n",
    "#      [ 97.4, 114.9, 104.1, 100. , 109.1, 106.7,  87.8, 101.1, 113.9,109.4]]):\n",
    "      print(f\"########### OD: {i} ###########\")\n",
    "      print(x)\n",
    "      \n",
    "      Path(f'{simulation_run_path}/initial_search').mkdir(parents=True, exist_ok=True)\n",
    "      new_od_xml = f'{simulation_run_path}/initial_search/gt_od_{gt_version_str}_{i}.xml'\n",
    "      prefix_output_init = f'{simulation_run_path}/initial_search/sobol_{i}'\n",
    "\n",
    "      # Generate OD\n",
    "      #curr_od = gt_od_vals.copy()\n",
    "      curr_od = np.array(x)\n",
    "\n",
    "      print(f'total expected GT demand: {np.sum(curr_od)}')\n",
    "\n",
    "      ###\n",
    "      # create OD xml file \n",
    "      ###\n",
    "      base_od['count'] = curr_od\n",
    "      # round to 1 decimal point\n",
    "      base_od['count'] = [round(elem, 1) for elem in base_od['count']]     \n",
    "      base_od = base_od.rename(columns={'fromTaz':'from', 'toTaz':'to'})        \n",
    "      create_taz_xml(new_od_xml, base_od, od_duration_sec, base_path)\n",
    "      ods_epsilon.append(curr_od)\n",
    "\n",
    "      # simulate gt od\n",
    "      simulate_od(new_od_xml, \n",
    "                  prefix_output_init, \n",
    "                  base_path, \n",
    "                  net_xml, \n",
    "                  taz2edge_xml, \n",
    "                  additional_xml, \n",
    "                  routes_df,\n",
    "                  sim_end_time,\n",
    "                  TRIPS2ODS_OUT_STR)\n",
    "\n",
    "      ## Compute loss\n",
    "      #prefix_output = f'initial_search/sobol_{i}'\n",
    "      sim_edge_out = f'{base_path}/{prefix_output_init}_{EDGE_OUT_STR}'\n",
    "      print(sim_edge_out)\n",
    "      curr_loop_stats, _, _ = parse_loop_data_xml_to_pandas(base_path, sim_edge_out,prefix_output_init,SUMO_PATH)\n",
    "      curr_loss = compute_nrmse_counts_all_edges(gt_edge_data, curr_loop_stats)\n",
    "\n",
    "      loss_all.append(curr_loss)\n",
    "      print(f\"############## loss: {curr_loss} ##############\")\n",
    "\n",
    "      # Parse training data\n",
    "      df_curr = pd.DataFrame(curr_od.reshape(1,dim_od),\n",
    "                        columns = [f\"x_{i+1}\" for i in range(dim_od)])\n",
    "      df_curr['loss'] = curr_loss\n",
    "      batch_data_i.append(df_curr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_initial_bo = pd.concat(batch_data_i)\n",
    "df_initial_bo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulation_run_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save initial dataset\n",
    "df_initial_bo.to_csv(f\"{simulation_run_path}/initial_search/data_set_ods_0_2000.csv\",index=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bayesian optimization helpers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def initialize_gp_model(train_X,train_Y):\n",
    "    \n",
    "    dim = train_X.size(dim=1)\n",
    "\n",
    "    likelihood = GaussianLikelihood(noise_constraint=Interval(1e-8, 1e-3))\n",
    "    covar_module = ScaleKernel(  # Use the same lengthscale prior as in the TuRBO paper\n",
    "        MaternKernel(\n",
    "            nu=2.5, ard_num_dims=dim, lengthscale_constraint=Interval(0.005, 4.0)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    gp_model = SingleTaskGP(\n",
    "        train_X, train_Y, \n",
    "        covar_module=covar_module, likelihood=likelihood, \n",
    "        outcome_transform=Standardize(m=1)\n",
    "    )\n",
    "\n",
    "    gp_mll = ExactMarginalLogLikelihood(gp_model.likelihood, gp_model)\n",
    "    \n",
    "    return gp_model, gp_mll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Acquisition Function: q-EI\n",
    "# Acquisition function\n",
    "\n",
    "sampler = StochasticSampler(sample_shape=torch.Size([128])) # TODO : sample shape from sim_setup?\n",
    "#qEI = qExpectedImprovement(gp_model, best_f=max(train_Y), sampler=sampler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_acqf_and_get_observation(acq_func,bounds):\n",
    "    \"\"\"Optimizes the acquisition function, and returns a new candidate.\"\"\"\n",
    "\n",
    "    dim = acq_func.model.train_inputs[0].size(dim=1)\n",
    "\n",
    "    # optimize\n",
    "    candidates, _ = optimize_acqf(\n",
    "        acq_function=acq_func,\n",
    "        bounds=torch.tensor([[0.0] * dim, [1.0] * dim], device=device, dtype=dtype),\n",
    "        q=BATCH_SIZE,\n",
    "        num_restarts=NUM_RESTARTS,\n",
    "        raw_samples=RAW_SAMPLES,  # used for intialization heuristic\n",
    "        options={\"batch_limit\": 5, \"maxiter\": 200},\n",
    "    )\n",
    "\n",
    "    # observe new values \n",
    "    new_x = candidates.detach()\n",
    "    \n",
    "    return unnormalize(new_x, bounds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_0 = pd.read_csv(base_path + f\"/initial_search/data_set_ods_0_2000.csv\")\n",
    "df_0 = pd.read_csv(f\"{simulation_run_path}/initial_search/data_set_ods_0_2000.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#/Users/rodrse/Downloads/ForSergio_nov26_2023_final/bayesian_optimization/vanilla_bo\n",
    "### Run loop\n",
    "best_value = []\n",
    "\n",
    "# Data frame of current training data\n",
    "df_training = df_0\n",
    "df_training[\"bo_iteration\"] = 0\n",
    "\n",
    "df_edge_stats = pd.DataFrame()\n",
    "\n",
    "#num_epsilon_iter = 2\n",
    "bayes_opt_method = \"bayesian_optimization/vanilla_bo_experiment_2\"\n",
    "ods_epsilon = []\n",
    "loss_all = []\n",
    "batch_data_i = []\n",
    "\n",
    "# Base OD which we will update their count entries\n",
    "base_od = gt_od_df.copy()\n",
    "gt_od_vals = gt_od_df['count'].astype(float).to_numpy()\n",
    "\n",
    "for i in range(NITER):\n",
    "\n",
    "    # new_od_xml = f'{bayes_opt_method}/gt_od_{gt_version_str}_{i}.xml'\n",
    "    # prefix_output = f'{bayes_opt_method}/bayesOpt_{i}'\n",
    "\n",
    "    new_od_xml = f'{simulation_run_path}/od.xml'\n",
    "    prefix_output_bo = f'{simulation_run_path}/BO/bayesOpt_{i}'\n",
    "    Path(f'{simulation_run_path}/BO').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    ########\n",
    "    # Start BO step\n",
    "    ########\n",
    "\n",
    "    print(f\"########### BO iteration={i+1} ###########\")\n",
    "\n",
    "    # Obtain sampling locations x\n",
    "    train_X = torch.from_numpy(\n",
    "        df_training[[col for col in df_training.columns if \"x\" in col]].values\n",
    "    ).to(device=device, dtype=dtype)\n",
    "\n",
    "    # Normalize\n",
    "    train_X_norm = normalize(train_X,bounds)\n",
    "\n",
    "    # Obtain reponse data\n",
    "    train_Y = -torch.from_numpy(df_training[[\"loss\"]].values) # Take negative\n",
    "\n",
    "    # best value so far\n",
    "    best_y = train_Y.max()\n",
    "    best_value.append(best_y)\n",
    "    print(f\"##### best_value={best_y} #####\")\n",
    "\n",
    "    print(f\"Generating new sampling location(s)....\")\n",
    "    # Declare model with newest data\n",
    "    gp_model, gp_mll = initialize_gp_model(train_X_norm,train_Y)\n",
    "\n",
    "    # Fit model\n",
    "    fit_gpytorch_mll(gp_mll)\n",
    "\n",
    "    # Construct acquistion function \n",
    "    sampler = StochasticSampler(sample_shape=torch.Size([128]))\n",
    "    # qEI = qExpectedImprovement(gp_model, best_f=best_y, sampler=sampler)\n",
    "    qEI = qLogExpectedImprovement(gp_model, best_f=best_y, sampler=sampler)\n",
    "\n",
    "    # Maximize acquisition function to get next observation\n",
    "    x_i = optimize_acqf_and_get_observation(acq_func=qEI,bounds=bounds)\n",
    "\n",
    "    # map the normalized into the original parameter space\n",
    "    #x_i = unnormalize(x_i, bounds)\n",
    "    x_i = x_i.cpu().detach().numpy()\n",
    "\n",
    "    ########\n",
    "    # End BO step\n",
    "    ########    \n",
    "\n",
    "\n",
    "    # Sample simulator (inner loop across all sampling locations within a batch)\n",
    "    # TODO: Parallelize\n",
    "    batch_data_i = []\n",
    "    for j in range(BATCH_SIZE):\n",
    "        loss_all = []\n",
    "        print(f\"########### Sampling location={j+1} ###########\")\n",
    "\n",
    "        # Generate OD\n",
    "        #curr_od = gt_od_vals.copy()\n",
    "        curr_od = x_i[j]\n",
    "\n",
    "        print(f'total expected GT demand: {np.sum(curr_od)}')\n",
    "\n",
    "        base_od['count'] = curr_od\n",
    "        # round to 1 decimal point\n",
    "        base_od['count'] = [round(elem, 1) for elem in base_od['count']]     \n",
    "        base_od = base_od.rename(columns={'fromTaz':'from', 'toTaz':'to'})        \n",
    "        create_taz_xml(new_od_xml, base_od, od_duration_sec, base_path)\n",
    "\n",
    "        # simulate gt od\n",
    "        simulate_od(new_od_xml, \n",
    "                    prefix_output_bo, \n",
    "                    base_path, \n",
    "                    net_xml, \n",
    "                    taz2edge_xml, \n",
    "                    additional_xml, \n",
    "                    routes_df,\n",
    "                    sim_end_time,\n",
    "                    TRIPS2ODS_OUT_STR)\n",
    "\n",
    "        ## Compute loss\n",
    "        sim_edge_out = f'{base_path}/{prefix_output_bo}_{EDGE_OUT_STR}'\n",
    "        print(sim_edge_out)\n",
    "        curr_loop_stats, _, _ = parse_loop_data_xml_to_pandas(base_path, sim_edge_out,prefix_output_bo,SUMO_PATH)\n",
    "        \n",
    "        curr_loss = compute_nrmse_counts_all_edges(gt_edge_data, curr_loop_stats)\n",
    "        loss_all.append(curr_loss)\n",
    "        print(f\"############## loss: {curr_loss} ##############\")\n",
    "\n",
    "        # Parse training data\n",
    "        df_j = pd.DataFrame(x_i[j].reshape(1,dim_od),\n",
    "                            columns = [f\"x_{i+1}\" for i in range(dim_od)])\n",
    "        df_j['loss'] = curr_loss\n",
    "        batch_data_i.append(df_j)\n",
    "\n",
    "        curr_loop_stats['bo_iteration'] = i\n",
    "        curr_loop_stats['batch'] = j\n",
    "        df_edge_stats = pd.concat([df_edge_stats, curr_loop_stats])\n",
    "\n",
    "    df_i = pd.concat(batch_data_i)\n",
    "    df_i[\"bo_iteration\"] = i+1\n",
    "\n",
    "    df_training = pd.concat([df_training,df_i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(base_path + f\"/{bayes_opt_method}/data_set_bayes_opt.csv\")\n",
    "print(f\"{simulation_run_path}/BO/data_set_bayes_opt.csv\")\n",
    "# df_training.to_csv(base_path + f\"/{bayes_opt_method}/data_set_bayes_opt.csv\",index=None)\n",
    "df_training.to_csv(f\"{simulation_run_path}/BO/data_set_bayes_opt.csv\",index=None)\n",
    "\n",
    "print(f\"{simulation_run_path}/BO/df_edge_stats.csv\")\n",
    "df_edge_stats.to_csv(f\"{simulation_run_path}/BO/df_edge_stats.csv\",index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot trajectories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training = pd.read_csv(f\"{simulation_run_path}/BO/data_set_bayes_opt.csv\")\n",
    "df_edge_stats = pd.read_csv(f\"{simulation_run_path}/BO/df_edge_stats.csv\")\n",
    "\n",
    "df_plot = df_training.query('bo_iteration>0')\n",
    "x = df_plot['bo_iteration']\n",
    "y = df_plot['loss'].cummin()\n",
    "\n",
    "plt.plot(x, y)\n",
    "#plt.legend(title='Parameter where:')\n",
    "plt.xlabel('BO epochs')\n",
    "plt.ylabel('Best NRMSE')\n",
    "# plt.show()\n",
    "plt.savefig(f\"{simulation_run_path}/bo_nrmse.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df_edge_stats.batch.drop_duplicates().shape[0] > 1:\n",
    "    raise('This needs updating once we start using batches')\n",
    "\n",
    "losses = []\n",
    "for o1 in range(NITER): #num_epsilon_iter):\n",
    "    curr_edge_stats = df_edge_stats[df_edge_stats.bo_iteration == o1]\n",
    "    df1b = gt_edge_data.merge(curr_edge_stats, on=['edge_id'], how='left', suffixes=('_gt', '_bo'))\n",
    "    curr_loss = compute_nrmse_counts_all_edges(gt_edge_data, curr_edge_stats)\n",
    "    losses.append(curr_loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disable interactive mode\n",
    "plt.ioff()\n",
    "\n",
    "if df_edge_stats.batch.drop_duplicates().shape[0] > 1:\n",
    "    raise('This needs updating once we start using batches')\n",
    "\n",
    "Path(f\"{simulation_run_path}/figs\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "losses = []\n",
    "for o1 in range(NITER): #num_epsilon_iter):\n",
    "    curr_edge_stats = df_edge_stats[df_edge_stats.bo_iteration == o1]\n",
    "    df1b = gt_edge_data.merge(curr_edge_stats, on=['edge_id'], how='left', suffixes=('_gt', '_bo'))\n",
    "    curr_loss = compute_nrmse_counts_all_edges(gt_edge_data, curr_edge_stats)\n",
    "    losses.append(curr_loss)\n",
    "\n",
    "    # find idx of min loss\n",
    "    idx_min = np.argmin(losses)\n",
    "    o1 = idx_min\n",
    "\n",
    "    curr_edge_stats = df_edge_stats[df_edge_stats.bo_iteration == o1]\n",
    "    df1b = gt_edge_data.merge(curr_edge_stats, on=['edge_id'], how='left', suffixes=('_gt', '_bo'))\n",
    "    curr_loss = compute_nrmse_counts_all_edges(gt_edge_data, curr_edge_stats)\n",
    "\n",
    "    plt.figure()    \n",
    "    # plotting diagonal line that represents a perfect data fit\n",
    "    max_val = np.max([df1b.interval_nVehContrib_gt.max(), df1b.interval_nVehContrib_bo.max()])\n",
    "    vec = np.arange(max_val)\n",
    "    plt.plot(vec, vec, 'r-')\n",
    "    plt.plot(df1b.interval_nVehContrib_gt, df1b.interval_nVehContrib_bo, 'x') \n",
    "    # plt.title(f'BO epochs: {o1}; loss: {curr_loss}')\n",
    "    plt.xlabel('GT edge counts') \n",
    "    plt.ylabel('Simulated edge counts') \n",
    "    plt.savefig(f\"{simulation_run_path}/figs/{o1}_bo_edge_counts.png\")\n",
    "\n",
    "\n",
    "    # plot of fit to GT OD vs ET OD\n",
    "    plt.figure()\n",
    "    # get the OD values with the best loss\n",
    "    curr_od = df_training.query('bo_iteration==@o1').iloc[0][[col for col in df_training.columns if \"x\" in col]].values\n",
    "    # bar graph side by side by x axis\n",
    "    width = 0.35\n",
    "    plt.bar(np.arange(len(curr_od)), curr_od, width, label='BO')\n",
    "    plt.bar(np.arange(len(gt_od_vals)) + width, gt_od_vals, width, label='GT')\n",
    "    plt.legend()\n",
    "    plt.xlabel('OD pair')\n",
    "    plt.ylabel('Demand')\n",
    "    # plt.title(f'BO iteration: {o1}')\n",
    "    plt.savefig(f\"{simulation_run_path}/figs/{o1}_bo_od.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw a joint seaborn plot for initial (o1 = 0) and best (o1 = idx_min) iteration\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "o1 = 0\n",
    "curr_edge_stats = df_edge_stats[df_edge_stats.bo_iteration == o1]\n",
    "df1b = gt_edge_data.merge(curr_edge_stats, on=['edge_id'], how='left', suffixes=('_gt', '_bo'))\n",
    "curr_loss = compute_nrmse_counts_all_edges(gt_edge_data, curr_edge_stats)\n",
    "\n",
    "o2 = idx_min\n",
    "curr_edge_stats = df_edge_stats[df_edge_stats.bo_iteration == o2]\n",
    "df2b = gt_edge_data.merge(curr_edge_stats, on=['edge_id'], how='left', suffixes=('_gt', '_bo'))\n",
    "\n",
    "# define another df to draw joint plot // hue = bo_iteration\n",
    "df1b['bo_iteration'] = o1\n",
    "df2b['bo_iteration'] = o2\n",
    "df1b['type'] = 'initial'\n",
    "df2b['type'] = 'best'\n",
    "\n",
    "df_joint = pd.concat([df1b, df2b])\n",
    "\n",
    "# plotting diagonal line that represents a perfect data fit\n",
    "max_val = np.max([df1b.interval_nVehContrib_gt.max(), df1b.interval_nVehContrib_bo.max()])\n",
    "vec = np.arange(max_val)\n",
    "\n",
    "g = sns.jointplot(data=df_joint, x='interval_nVehContrib_gt', y='interval_nVehContrib_bo', hue='type')\n",
    "# x label\n",
    "g.ax_joint.set_xlabel('GT edge counts')\n",
    "# y label\n",
    "g.ax_joint.set_ylabel('Simulated edge counts')\n",
    "g.ax_joint.plot(vec, vec, 'r-')\n",
    "\n",
    "# save the plot\n",
    "plt.savefig(f\"{simulation_run_path}/{network_name}_jointplot_initial_best.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
